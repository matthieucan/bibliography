@misc{wifi-attacks-wep-wpa,
  author = {Caneill, Matthieu and Jean-Loup Gilis},
  title = {Attacks against the WiFi protocols WEP and WPA},
  abstract = {Wireless networks are today an entire part of the Internet, and are often used by companies and particularies. Security of information is thus important, and protocols like WEP and WPA can be attacked. We present in this report the existent WLANs protocols, an overview of the most efficient attacks on them and two attacks we have found on WEP.},
  year = {2010}
}

@inproceedings{debsources-esem-2014,
  author = {Caneill, Matthieu and Stefano Zacchiroli},
  title = {Debsources: Live and Historical Views on Macro-Level Software Evolution},
  abstract = {Context. Software evolution has been an active field of research in recent years, but studies on macro-level software evolution---i.e., on the evolution of large software collections over many years---are scarce, despite the increasing popularity of intermediate vendors as a way to deliver software to final users. Goal. We want to ease the study of both day-by-day and long-term Free and Open Source Software (FOSS) evolution trends at the macro-level, focusing on the Debian distribution as a proxy of relevant FOSS projects. Method. We have built Debsources, a software platform to gather, search, and publish on the Web all the source code of Debian and measures about it. We have set up a public Debsources instance at http://sources.debian.net, integrated it into the Debian infrastructure to receive live updates of new package releases, and written plugins to compute popular source code metrics. We have injected all current and historical Debian releases into it. Results. The obtained dataset and Web portal provide both long term-views over the past 20 years of FOSS evolution and live insights on what is happening at sub-day granularity. By writing simple plugins (~100 lines of Python each) and adding them to our Debsources instance we have been able to easily replicate and extend past empirical analyses on metrics as diverse as lines of code, number of packages, and rate of change---and make them perennial. We have obtained slightly different results than our reference study, but confirmed the general trends and updated them in light of 7 extra years of evolution history. Conclusions. Debsources is a flexible platform to monitor large FOSS collections over long periods of time. Its main instance and dataset are valuable resources for scholars interested in macro-level software evolution.},
  publisher = {ACM},
  year = {2014},
  isbn = {978-1-4503-2774-9},
  doi = {10.1145/2652524.2652528},
  booktitle = {ESEM 2014: 8th International Symposium on Empirical Software Engineering and Measurement},
}

@article{debsources-ese-2016,
  author = {Caneill, Matthieu and Daniel M. Germán and Stefano Zacchiroli},
  title = {The Debsources Dataset: Two Decades of Free and Open Source Software},
  abstract = {We present the Debsources Dataset: source code and related metadata spanning two decades of Free and Open Source Software (FOSS) history, seen through the lens of the Debian distribution. The dataset spans more than 3 billion lines of source code as well as metadata about them such as: size metrics (lines of code, disk usage), developer-defined symbols (ctags), file-level checksums (SHA1, SHA256, TLSH), file media types (MIME), release information (which version of which package containing which source code files has been released when), and license information (GPL, BSD, etc). The Debsources Dataset comes as a set of tarballs containing deduplicated unique source code files organized by their SHA1 checksums (the source code), plus a portable PostgreSQL database dump (the metadata). A case study is run to show how the Debsources Dataset can be used to easily and efficiently instrument very long-term analyses of the evolution of Debian from various angles (size, granularity, licensing, etc.), getting a grasp of major FOSS trends of the past two decades. The Debsources Dataset is Open Data, released under the terms of the CC BY-SA 4.0 license, and available for download from Zenodo with DOI reference 10.5281/zenodo.61089.},
  publisher = {Springer},
  year = {2016},
  issn = {1382-3256},
  doi = {10.1007/s10664-016-9461-5},
  journal = {Empirical Software Engineering},
}

@inproceedings{storm-locality-middleware-2016,
 author = {Caneill, Matthieu and El Rheddane, Ahmed and Leroy, Vincent and De Palma, No\"{e}l},
 title = {Locality-Aware Routing in Stateful Streaming Applications},
 abstract = {Distributed stream processing engines continuously execute series of operators on data streams. Horizontal scaling is achieved by deploying multiple instances of each operator in order to process data tuples in parallel. As the application is distributed on an increasingly high number of servers, the likelihood that the stream is sent to a different server for each operator increases. This is particularly important in the case of stateful applications that rely on keys to deterministically route messages to a specific instance of an operator. Since network is a bottleneck for many stream applications, this behavior significantly degrades their performance. Our objective is to improve stream locality for stateful stream processing applications. We propose to analyse traces of the application to uncover correlations between the keys used in successive routing operations. By assigning correlated keys to instances hosted on the same server, we significantly reduce network consumption and increase performance while preserving load balance. Furthermore, this approach is executed online, so that the assignment can automatically adapt to changes in the characteristics of the data. Data migration is handled seamlessly with each routing configuration update. We implemented and evaluated our protocol using Apache Storm, with a real workload consisting of geo-tagged Flickr pictures as well as Twitter publications. Our results show a significant improvement in throughput.},
 booktitle = {Proceedings of the 17th International Middleware Conference},
 series = {Middleware '16},
 year = {2016},
 isbn = {978-1-4503-4300-8},
 location = {Trento, Italy},
 pages = {4:1--4:13},
 articleno = {4},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/2988336.2988340},
 doi = {10.1145/2988336.2988340},
 acmid = {2988340},
 publisher = {ACM},
 keywords = {network locality, optimization, routing, stream processing},
}

@phdthesis{2018-phd-thesis-caneill,
  author       = {Matthieu Caneill},
  title        = {Contributions to large-scale data processing systems},
  school       = {Univ. Grenoble Alpes},
  year         = 2018,
  month        = 2,
  abstract     = {This thesis covers the topic of large-scale data processing systems,
and more precisely three complementary approaches: the design of a
system to perform prediction about computer failures through the
analysis of monitoring data; the routing of data in a real-time system
looking at correlations between message fields to favor locality; and
finally a novel framework to design data transformations using
directed graphs of blocks.

Through the lenses of the Smart Support Center project, we design a
scalable architecture, to store time series reported by monitoring
engines, which constantly check the health of computer systems. We use
this data to perform predictions, and detect potential problems before
they arise.

We then dive in routing algorithms for stream processing systems, and
develop a layer to route messages more efficiently, by avoiding hops
between machines. For that purpose, we identify in real-time the
correlations which appear in the fields of these messages, such as
hashtags and their geolocation, for example in the case of tweets. We
use these correlations to create routing tables which favor the
co-location of actors handling these messages.

Finally, we present λ-blocks, a novel programming framework to compute
data processing jobs without writing code, but rather by creating
graphs of blocks of code. The framework is fast, and comes with
batteries included: block libraries, plugins, and APIs to extend
it. It is also able to manipulate computation graphs, for
optimization, analysis, verification, or any other purposes.}
}